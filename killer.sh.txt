export do="--dry-run=client -o yaml"
echo "autocmd FileType yaml setlocal ai ts=2 sw=2 et" > /home/k8s/.vimrc

k8s@terminal:~$ kubectl config use-context k8s-c1-H	##Set requested context 
Switched to context "k8s-c1-H".
k8s@terminal:~$ kubectl config current-context 		##Verify current context
k8s-c1-H
k8s@terminal:~$ 

k8s@terminal:~$ kubectl get pods -n project-c13
NAME                                    READY   STATUS    RESTARTS   AGE
o3db-0                                  1/1     Running   0          45d
k8s@terminal:~$ kubectl get pods -n project-c13 --show-labels 
NAME                                    READY   STATUS    RESTARTS   AGE   LABELS
o3db-0                                  1/1     Running   0          45d   app=nginx,controller-revision-hash=o3db-5fbd4bb9cc,statefulset.kubernetes.io/pod-name=o3db-0
k8s@terminal:~$ kubectl scale -n project-c13 statefulset o3db --replicas=1 --record 
statefulset.apps/o3db scaled
k8s@terminal:~$ 
##########

k8s@terminal:~$ cat pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: safari-pv
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /Volumes/Data    

k8s@terminal:~$ cat pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: safari-pvc
  namespace: project-tiger
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi

k8s@terminal:~$ cat safari.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: safari
  name: safari
  namespace: project-tiger
spec:
  replicas: 1
  selector:
    matchLabels:
      app: safari
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: safari
    spec:
      containers:
      - image: httpd:2.4.41-alpine
        name: httpd
        volumeMounts:
        - mountPath: /tmp/safari-data 
          name: site-data
      volumes:
        - name: site-data
          persistentVolumeClaim:
            claimName: safari-pvc 
k8s@terminal:~$ 
##########

k8s@terminal:~$ kubectl config get-contexts
CURRENT   NAME         CLUSTER      AUTHINFO     NAMESPACE
*         k8s-c1-H     k8s-c1-H     k8s-c1-H     
          k8s-c2-AC    k8s-c2-AC    k8s-c2-AC    
          k8s-c3-CCC   k8s-c3-CCC   k8s-c3-CCC   
k8s@terminal:~$ kubectl config get-contexts -o name
k8s-c1-H
k8s-c2-AC
k8s-c3-CCC
k8s@terminal:~$ kubectl config get-contexts -o name > /opt/course/1/contexts
k8s@terminal:~$ kubectl config current-context 
k8s-c1-H
k8s@terminal:~$ kubectl config current-context > /opt/course/1/context_default_kubectl.sh
k8s@terminal:~$ cat .kube/config | grep -i current 
current-context: k8s-c1-H
k8s@terminal:~$ cat .kube/config | grep -i current | awk '{print $1}'
current-context:
k8s@terminal:~$ cat .kube/config | grep -i current | awk '{print $2}'
k8s-c1-H
k8s@terminal:~$ cat .kube/config | grep -i current | awk '{print $2}' > /opt/course/1/context_default_no_kubectl.sh
k8s@terminal:~$ 
##########

k8s@terminal:~$ kubectl explain pods.spec.nodeSelector
KIND:     Pod
VERSION:  v1

FIELD:    nodeSelector <map[string]string>

DESCRIPTION:
     NodeSelector is a selector which must be true for the pod to fit on a node.
     Selector which must match a node's labels for the pod to be scheduled on
     that node. More info:
     https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
k8s@terminal:~$ 
##########

root@cluster2-master1:~# ls -ltrh /etc/kubernetes/manifests/*
-rw------- 1 root root 3.3K May  4 10:48 /etc/kubernetes/manifests/kube-controller-manager.yaml
-rw------- 1 root root 3.8K May  4 10:48 /etc/kubernetes/manifests/kube-apiserver.yaml
-rw------- 1 root root 2.2K May  4 10:48 /etc/kubernetes/manifests/etcd.yaml
root@cluster2-master1:~# ls -ltrh /etc/kubernetes/kube
kubelet.conf         kube-scheduler.yaml  
root@cluster2-master1:~# ls -ltrh /etc/kubernetes/kube-scheduler.yaml 
-rw------- 1 root root 1.4K May  4 10:48 /etc/kubernetes/kube-scheduler.yaml	##scheduler killed / mv config yaml aside !!
root@cluster2-master1:~# cat manual-schedule.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: manual-schedule
  name: manual-schedule
spec:
  nodeName: cluster2-master1	##manual-schedule of pod on master!! 
  containers:
  - image: httpd:2.4-alpine
    name: manual-schedule
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
root@cluster2-master1:~# kubectl get pods -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP          NODE               NOMINATED NODE   READINESS GATES
manual-schedule   1/1     Running   0          95s   10.32.0.4   cluster2-master1   <none>           <none>
root@cluster2-master1:~# 

##########
k8s@terminal:~$ kubectl create role processor --resource=Secrets,ConfigMaps --verb=create --namespace=project-hamster 
role.rbac.authorization.k8s.io/processor created
k8s@terminal:~$ kubectl create rolebinding processor --role=processor --serviceaccount=processor
error: serviceaccount must be <namespace>:<name>
k8s@terminal:~$ kubectl create rolebinding processor --role=processor --serviceaccount=project-hamster:processor
rolebinding.rbac.authorization.k8s.io/processor created
k8s@terminal:~$ 

##########
root@cluster1-master1:/etc/kubernetes/manifests# cat kube-apiserver.yaml | grep -i range
    - --service-cluster-ip-range=10.96.0.0/12		##service range (CIDR)
root@cluster1-master1:/etc/kubernetes/manifests# exit
##########

#Upgrade the kubelet and kubectl

root@cluster3-master1:~# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.1", GitCommit:"632ed300f2c34f6d6d15ca4cef3d3c7073412212", GitTreeState:"clean", BuildDate:"2021-08-19T15:44:22Z", GoVersion:"go1.16.7", Compiler:"gc", Platform:"linux/amd64"}
root@cluster3-master1:~# kubectl version
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.1", GitCommit:"632ed300f2c34f6d6d15ca4cef3d3c7073412212", GitTreeState:"clean", BuildDate:"2021-08-19T15:45:37Z", GoVersion:"go1.16.7", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.1", GitCommit:"632ed300f2c34f6d6d15ca4cef3d3c7073412212", GitTreeState:"clean", BuildDate:"2021-08-19T15:39:34Z", GoVersion:"go1.16.7", Compiler:"gc", Platform:"linux/amd64"}
root@cluster3-master1:~# kubelet --version
Kubernetes v1.22.1
root@cluster3-master1:~# exit

#apt-mark unhold kubelet kubectl; apt-get update; apt-get install -y kubelet=1.22.1-00 kubectl=1.22.1-00; apt-mark hold kubelet kubectl
#systemctl daemon-reload; systemctl restart kubelet

root@cluster3-worker2:~# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.1", GitCommit:"632ed300f2c34f6d6d15ca4cef3d3c7073412212", GitTreeState:"clean", BuildDate:"2021-08-19T15:44:22Z", GoVersion:"go1.16.7", Compiler:"gc", Platform:"linux/amd64"}
root@cluster3-worker2:~# kubectl version
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.1", GitCommit:"632ed300f2c34f6d6d15ca4cef3d3c7073412212", GitTreeState:"clean", BuildDate:"2021-08-19T15:45:37Z", GoVersion:"go1.16.7", Compiler:"gc", Platform:"linux/amd64"}
The connection to the server localhost:8080 was refused - did you specify the right host or port?
root@cluster3-worker2:~# kubelet --version
Kubernetes v1.22.1
root@cluster3-worker2:~# 

root@cluster3-master1:~# kubeadm token create --print-join-command
kubeadm join 192.168.100.31:6443 --token z2r2cc.pq2nlpq13b9zjfhn --discovery-token-ca-cert-hash sha256:2e2c3407a256fc768f0d8e70974a8e24d7b9976149a79bd08858c4d7aa2ff79a 
root@cluster3-master1:~# 
##########

k8s@terminal:~$ kubectl get nodes
NAME               STATUS     ROLES                  AGE   VERSION
cluster3-master1   Ready      control-plane,master   91d   v1.22.1
cluster3-worker1   NotReady   <none>                 91d   v1.22.1
cluster3-worker2   Ready      <none>                 23m   v1.22.1
k8s@terminal:~$ ssh cluster3-worker1
Last login: Thu Dec 16 03:15:37 2021 from 192.168.100.2
root@cluster3-worker1:~# ps -ef | grep -i kubelet | grep -v grep
root@cluster3-worker1:~# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Thu 2021-12-16 03:29:07 UTC; 4s ago
       Docs: https://kubernetes.io/docs/home/
    Process: 22766 ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=203/EXEC)
   Main PID: 22766 (code=exited, status=203/EXEC)

Dec 16 03:29:07 cluster3-worker1 systemd[1]: kubelet.service: Main process exited, code=exited, status=203/EXEC
Dec 16 03:29:07 cluster3-worker1 systemd[1]: kubelet.service: Failed with result 'exit-code'.
root@cluster3-worker1:~# systemctl start kubelet
root@cluster3-worker1:~# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Thu 2021-12-16 03:29:28 UTC; 2s ago
       Docs: https://kubernetes.io/docs/home/
    Process: 22793 ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=203/EXEC)
   Main PID: 22793 (code=exited, status=203/EXEC)

Dec 16 03:29:28 cluster3-worker1 systemd[1]: kubelet.service: Failed with result 'exit-code'.
root@cluster3-worker1:~# 

root@cluster3-worker1:~# journalctl -u kubelet.service | head
-- Logs begin at Wed 2021-09-15 15:12:51 UTC, end at Thu 2021-12-16 03:31:20 UTC. --
Sep 15 15:14:17 cluster3-worker1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Sep 15 15:14:17 cluster3-worker1 systemd[1]: kubelet.service: Current command vanished from the unit file, execution of the command list won't be resumed.
Sep 15 15:14:18 cluster3-worker1 systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Sep 15 15:14:18 cluster3-worker1 systemd[1]: kubelet.service: Succeeded.
Sep 15 15:14:18 cluster3-worker1 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Sep 15 15:14:18 cluster3-worker1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Sep 15 15:14:18 cluster3-worker1 kubelet[4876]: E0915 15:14:18.496043    4876 server.go:206] "Failed to load kubelet config file" err="failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file \"/var/lib/kubelet/config.yaml\", error: open /var/lib/kubelet/config.yaml: no such file or directory" path="/var/lib/kubelet/config.yaml"
Sep 15 15:14:18 cluster3-worker1 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Sep 15 15:14:18 cluster3-worker1 systemd[1]: kubelet.service: Failed with result 'exit-code'.
root@cluster3-worker1:~# journalctl -u kubelet.service
-- Logs begin at Wed 2021-09-15 15:12:51 UTC, end at Thu 2021-12-16 03:31:41 UTC. --
.
..
Dec 16 03:10:40 cluster3-worker1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Dec 16 03:10:40 cluster3-worker1 systemd[20756]: kubelet.service: Failed to execute command: No such file or directory
Dec 16 03:10:40 cluster3-worker1 systemd[20756]: kubelet.service: Failed at step EXEC spawning /usr/local/bin/kubelet: No such file or directory
Dec 16 03:10:40 cluster3-worker1 systemd[1]: kubelet.service: Main process exited, code=exited, status=203/EXEC
Dec 16 03:10:40 cluster3-worker1 systemd[1]: kubelet.service: Failed with result 'exit-code'.
Dec 16 03:10:50 cluster3-worker1 systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 5.
Dec 16 03:10:50 cluster3-worker1 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Dec 16 03:10:50 cluster3-worker1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Dec 16 03:10:50 cluster3-worker1 systemd[20767]: kubelet.service: Failed to execute command: No such file or directory
Dec 16 03:10:50 cluster3-worker1 systemd[20767]: kubelet.service: Failed at step EXEC spawning /usr/local/bin/kubelet: No such file or directory
Dec 16 03:10:50 cluster3-worker1 systemd[1]: kubelet.service: Main process exited, code=exited, status=203/EXEC

root@cluster3-worker1:~# which kubelet
/usr/bin/kubelet
root@cluster3-worker1:~# whereis kubelet
kubelet: /usr/bin/kubelet
root@cluster3-worker1:~# 

root@cluster3-worker1:~# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS	##update the correct path!!
root@cluster3-worker1:~# 

root@cluster3-worker1:~# vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 
root@cluster3-worker1:~# systemctl daemon-reload; systemctl restart kubelet
root@cluster3-worker1:~# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Thu 2021-12-16 03:35:39 UTC; 13s ago
       Docs: https://kubernetes.io/docs/home/
   Main PID: 23254 (kubelet)
      Tasks: 12 (limit: 467)
     Memory: 109.5M
     CGroup: /system.slice/kubelet.service
             └─23254 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote >

Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.694297   23254 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernete>
Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.694738   23254 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-bin2\" (UniqueName: \"kubernetes.i>
Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.695258   23254 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"dbus\" (UniqueName: \"kubernetes.io/ho>
Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.697125   23254 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"machine-id\" (UniqueName: \"kubernetes>
Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.697996   23254 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernet>
Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.698330   23254 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rnznh\" (UniqueName: \>
Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.698902   23254 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes>
Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.699559   23254 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-bin\" (UniqueName: \"kubernetes.io>
Dec 16 03:35:46 cluster3-worker1 kubelet[23254]: I1216 03:35:46.699848   23254 reconciler.go:157] "Reconciler: start to sync state"
Dec 16 03:35:51 cluster3-worker1 kubelet[23254]: E1216 03:35:51.358438   23254 kubelet.go:2332] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Net>
lines 1-22/22 (END)

root@cluster3-worker1:~# kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the right host or port?
root@cluster3-worker1:~# exit
logout
Connection to cluster3-worker1 closed.
k8s@terminal:~$ scp -r .kube/ root@cluster3-worker1:/root
config_cluster3_transformed     
.
..
f26f3421570682bf53a6e9910381cd13                                                                                                                                              100%  789   638.0KB/s   00:00    
config_cluster1_transformed                                                                                                                                                   100% 5580   787.0KB/s   00:00    
k8s@terminal:~$ !ssh
ssh cluster3-worker1
Last login: Thu Dec 16 03:28:41 2021 from 192.168.100.2
root@cluster3-worker1:~# kubectl get nodes
NAME               STATUS   ROLES                  AGE   VERSION
cluster3-master1   Ready    control-plane,master   91d   v1.22.1
cluster3-worker1   Ready    <none>                 91d   v1.22.1
cluster3-worker2   Ready    <none>                 31m   v1.22.1
root@cluster3-worker1:~# exit
logout
Connection to cluster3-worker1 closed.
k8s@terminal:~$ kubectl get nodes
NAME               STATUS   ROLES                  AGE   VERSION
cluster3-master1   Ready    control-plane,master   91d   v1.22.1
cluster3-worker1   Ready    <none>                 91d   v1.22.1
cluster3-worker2   Ready    <none>                 31m   v1.22.1
k8s@terminal:~$ 
